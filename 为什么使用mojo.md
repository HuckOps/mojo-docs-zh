## 为什么使用Mojo

当我们开始使用模块化时，我们并没有打算开发一种新的编程语言。但当我们在构建[platform to unify the world’s ML/AI infrastructure](https://www.modular.com/blog/the-case-for-a-next-generation-ai-developer-platform)时，我们意识到跨整个堆栈编程太复杂了。另外，我们手写了很多MLIR，很不开心。

我们想要的是一个创新的、可扩展的编程模型，它可以瞄准人工智能领域普遍存在的加速器和其他异构系统。这意味着一种编程语言具有强大的编译时元编程、自适应编译技术的集成、整个编译流程中的缓存以及现有语言不支持的其他特性。

尽管加速器很重要，但最普遍、有时被忽视的“加速器”之一是CPU。如今，cpu有许多tensor-core-like的加速块和其他AI加速单元，但它们也可以作为专用加速器无法处理的操作的“后备”，比如数据加载，前置或后置处理，和外部系统结合。所以很明显，我们不能仅仅依靠一种只与特定处理器一起工作的“加速器语言”来提升人工智能。

应用人工智能系统需要解决所有这些问题，我们没有理由认为不能只用一种语言来完成。于是，Mojo诞生了。

## 下一代编译技术语言

当我们意识到没有现有的语言可以解决人工智能计算的挑战，我们开始重新思考编程语言应该如何设计和实现来解决我们的问题。因为我们需要对各种各样的加速器提供高性能支持，传统的编译器技术，如LLVM和GCC并不适合(任何基于它们的语言和工具都是不够的)。虽然它们支持广泛的cpu和一些常用的gpu，但这些编译器技术是几十年前设计的，无法完全支持现代芯片架构。目前，标准的专用机器学习加速器技术是MLIR。

MLIR是一个相对较新的开源编译器基础设施，始于Google(Modular的引领着)，已经被广泛应用到机器学习加速器社区。MLIR的优势在于它能够构建特定领域的编译器，特别是对于不是传统cpu和gpu之外的奇怪领域，如AI ASICS、量子计算系统、fpga和定制芯片。

我们的目标是在Modular上构建下一代AI平台，我们已经在部分基础构建里使用了MLIR，但是我们没有一种变成语言可以释放MLIR在堆栈中全部潜力。虽然现在许多其他项目都在使用MLIR，但Mojo是第一个专门为MLIR设计的主要语言，这使得Mojo在为AI工作负载编写系统级代码时具有独特的功能。